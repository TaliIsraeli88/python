{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import collections\n",
    "from   collections import defaultdict\n",
    "import sys\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "#nltk.download()\n",
    "stemmer = PorterStemmer()\n",
    "token = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "dir = 'C:/Users/Yoav/Desktop/reuters/training/'\n",
    "queriesDir = 'C:/Users/Yoav/Desktop/HW2IRFinal/HW2IRFinal/input/queries.txt'\n",
    "resultDir = 'C:/Users/Yoav/Desktop/HW2IRFinal/HW2IRFinal/result.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Process - analyzeLine and buildLists functions :\n",
    "\n",
    "\n",
    "analyzeLine - This function get a line in input and return a ready line for index. \n",
    "Each line passes through the following stages:\n",
    "- tokenization\n",
    "- stemming\n",
    "- stop- words removal \n",
    "\n",
    "buildLists - Finally we save all words in 2 lists for the next steps:\n",
    "- words_list - unique values of all words in all documents together.\n",
    "- all_words_list - all words that appear in all documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyzeLine (line):\n",
    "    finalLine = [] \n",
    "    ans=[]\n",
    "    row = nltk.word_tokenize(line.lower()) \n",
    "    \n",
    "    #pre-process for each word\n",
    "    for word in row :\n",
    "        \n",
    "        #stemming\n",
    "        stemWord = stemmer.stem(word) \n",
    "        \n",
    "        #tokenize\n",
    "        if any(w.isdigit() for w in stemWord): \n",
    "            tokenWord = [stemWord] \n",
    "        else: \n",
    "            tokenWord = token.tokenize(stemWord)\n",
    "        \n",
    "        #stop- words removal \n",
    "        for wword in tokenWord:\n",
    "            if wword not in nltk.corpus.stopwords.words('english'):\n",
    "                finalLine.append(wword) #bulid final line\n",
    "    \n",
    "    return (finalLine)\n",
    "\n",
    "#build lists of words: \"words_list\" only unique values, and \"all_words_list\" include all words in documents  \n",
    "def buildLists (line):\n",
    "    for word in line: \n",
    "        if word not in words_list[fname]:\n",
    "            words_list[fname].append(word)\n",
    "        all_words_list[fname].append(word)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfIDFarrays and Inverted Index Functions:\n",
    "\n",
    "- The first function build all connected arrays that important for build the inverted index.\n",
    "Inverted index is calculated by tf*idf weights model:\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\"tf\" array save for each word the number of times it appear in each document. Both TF's parts are taken from \"tf\" array. \n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "Total number of documents - \"NumOfDocs\" variable.\n",
    "second part of idf is taken from \"DocPerTerm\" array, that keep for each word the number of documents it appears in it. \n",
    "The function also calculated for each doc it's vector length.\n",
    "\n",
    "- The second function build the appropriate inverted index, according to the documents in the input file. \n",
    "The calculate is for each word and document :tf*idf.\n",
    "the index is look like this : \"temporao: [0.0, 0.0]\". meaning this word is appear in 2 documents the same number of times this is why the weight is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfIDFarrays ():\n",
    "        \n",
    "    for key in words_list.keys():\n",
    "        for word in words_list[key]:\n",
    "            # tf array\n",
    "            tf[key].append(all_words_list[key].count(word))\n",
    "            \n",
    "            # calculated DocPerTerm array \n",
    "            if not word in DocPerTerm:\n",
    "                DocPerTerm[word] = 1\n",
    "            else:\n",
    "                DocPerTerm[word] += 1\n",
    "                \n",
    "    # calculate IDF array \n",
    "    for word in DocPerTerm :\n",
    "        idf[word] = math.log(float(NumOfDocs)/float(DocPerTerm[word]),10) \n",
    "        \n",
    "    for doc in words_list:\n",
    "        totalWords = len(all_words_list[doc])\n",
    "        fi=0\n",
    "        for word in words_list[doc]:\n",
    "            fi += math.pow(all_words_list[doc].count(word),2)/totalWords\n",
    "        VecPerDoc[doc] = fi\n",
    "        \n",
    "    return \n",
    "\n",
    "\n",
    "def invertedIndex():\n",
    "    \n",
    "    for key in words_list.keys():\n",
    "        word_enum = 0\n",
    "        for word in words_list[key]:\n",
    "            weightForIndex = {}\n",
    "            weightForIndex[key] = float(tf[key][word_enum])/float(max(tf[key]))\n",
    "            inverted_index[word].append(weightForIndex)\n",
    "            word_enum = word_enum + 1\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop - Part I:\n",
    "Read documents, analyze document and build the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_list = defaultdict(list)\n",
    "all_words_list = defaultdict(list)\n",
    "tf = defaultdict(list)\n",
    "inverted_index = defaultdict(list)\n",
    "DocPerTerm = defaultdict(list)\n",
    "VecPerDoc = defaultdict(list)\n",
    "idf = defaultdict(list)\n",
    "NumOfDocs = 0 \n",
    "\n",
    "for root, dirs, files in os.walk(dir):\n",
    "    for fname in files:\n",
    "        current_file = \"%s%s%s\" % (os.path.abspath(root), os.path.sep, fname)\n",
    "        f = open (current_file, 'r')\n",
    "        \n",
    "        for line in f:\n",
    "            row = [] \n",
    "            row = analyzeLine (line) #tokenize, stemming and stop-words for line and build words lists\n",
    "            buildLists (row)\n",
    "        \n",
    "    f.close ()\n",
    "    NumOfDocs = len(words_list)\n",
    "    tfIDFarrays () #calculate tf idf array \n",
    "    invertedIndex() #build inverted index\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Function:\n",
    "\n",
    "getDocResult function return a list of documantes with the similarity between query and documents. Output documents are ranked acording to aimilarity to query.\n",
    "\n",
    "First we analyze the query. Each query passes through the following stages:\n",
    "\n",
    "    tokenization\n",
    "    stemming\n",
    "    stop- words removal\n",
    "\n",
    "Finally we save all words in 2 lists for the next steps:\n",
    "\n",
    "    query_list - unique values of all words in query.\n",
    "    all_query_list - all words that appear in the query.\n",
    "\n",
    "Calculated Similarity:\n",
    "Cosine Similarity measures the consine of the angle between two vectors. First we calculated for the query the inner product = sum of tf(doc)*ifd(word in query). Then for each doc we calculated CosSim=inner priduct normilized by vector lenghts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDocResult(query):\n",
    "    query_words = defaultdict(list)\n",
    "    all_query_words = defaultdict(list)\n",
    "    res= defaultdict(list)\n",
    "    result= defaultdict(list)\n",
    "    finalQuery = []\n",
    "    totalQueryWords = 0\n",
    "    \n",
    "    # analyze Query\n",
    "    finalQuery = analyzeLine (query)  \n",
    "    \n",
    "    #build lists of words: \"query_words\" only unique values, and \"all_query_words\" include all words in documents           \n",
    "    for word in finalQuery: \n",
    "        \n",
    "        if word not in query_words[word]:\n",
    "            query_words[word].append(word)\n",
    "        all_query_words[word].append(word)\n",
    "    \n",
    "    #Count number of words in query\n",
    "    for word in all_query_words:\n",
    "        totalQueryWords =totalQueryWords + all_query_words[word].count(word)\n",
    "\n",
    "    # Calculated Similarity \n",
    "    for word in query_words:\n",
    "        for tf in inverted_index[word]:\n",
    "            for doc in tf:\n",
    "                if not doc in res:\n",
    "                    res[doc] = (float(tf[doc]) * float(idf[word]))\n",
    "                else:\n",
    "                    res[doc] += (float(tf[doc]) * float(idf[word]))\n",
    "\n",
    "        \n",
    "        for doc in res:\n",
    "            result[doc] = res[doc] / math.sqrt(totalQueryWords*VecPerDoc[doc]) \n",
    "            \n",
    "   \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function runQuery\n",
    "The function is getting a query and return the top 20 relevant documents ranked according to the Similarity measure.\n",
    "\n",
    "Function getTop20\n",
    "The function read a query file and write to a result file for each query the top 20 relevant documents ranked according to the Similarity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runQuery(query):\n",
    "    doc_result = defaultdict(list)\n",
    "    sort_doc_result = defaultdict(list)\n",
    "    \n",
    "    doc_result = getDocResult(query)\n",
    "    sort_doc_result = sorted(doc_result,key=doc_result.get,reverse=True)[:20]\n",
    "    \n",
    "    return (\";\").join(sort_doc_result)\n",
    "\n",
    "def getTop20():\n",
    "    results_file = open(resultDir, \"a\")\n",
    "    queries =  open(queriesDir,'r')\n",
    "    for query in queries: #run on each query\n",
    "        if query != '\\n':\n",
    "            results_file.write(runQuery(query) + '\\n')\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop - Part II:\n",
    "In order to create result output, pease run getTop20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getTop20()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
